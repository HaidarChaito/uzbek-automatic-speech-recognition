{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-05T13:21:52.322581Z",
     "start_time": "2026-02-05T13:21:52.318344Z"
    }
   },
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to Python path\n",
    "parent_dir = Path('.').absolute().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T13:21:58.106393Z",
     "start_time": "2026-02-05T13:21:52.426700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "DATASET_DIR = \"../datasets\"\n",
    "DATASET_PATH = os.path.join(DATASET_DIR, \"combined__case_sensitive_part2.csv\")\n",
    "SEED = 137\n",
    "\n",
    "OUTPUT_DIR = \"../outputs/part2\"\n",
    "PROCESSED_DATASET_DIR = os.path.abspath(os.path.join(OUTPUT_DIR, \"processed_uzbek_asr_dataset\"))\n",
    "\n",
    "MODEL_NAME = \"openai/whisper-small\""
   ],
   "id": "fcb9432595da9741",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T13:22:57.104263Z",
     "start_time": "2026-02-05T13:22:20.109646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=\"uz\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model.generation_config.language = \"uz\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Force decoder to generate in Uzbek\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"uz\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# Set pad token to avoid attention mask warning\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "print(processor.feature_extractor)\n",
    "model"
   ],
   "id": "650a28c5e837a62",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5b46fae1-f753-4917-964e-842d6213631a)')' thrown while requesting HEAD https://huggingface.co/openai/whisper-small/resolve/main/chat_template.jinja\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d008137c-4b8b-4a12-9b00-249fcb913b56)')' thrown while requesting HEAD https://huggingface.co/openai/whisper-small/resolve/main/chat_template.jinja\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d07df590-8262-4173-809e-ef17d55a7278)')' thrown while requesting HEAD https://huggingface.co/openai/whisper-small/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"dither\": 0.0,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T13:22:57.591388Z",
     "start_time": "2026-02-05T13:22:57.169503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import os\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "def prepare_dataset_for_training(batch):\n",
    "    \"\"\"Preprocess a batch for training\"\"\"\n",
    "    # Load and process audio from the 'audio' column\n",
    "    audios = batch[\"audio\"]\n",
    "\n",
    "    # Compute log-Mel input features\n",
    "    input_features = processor.feature_extractor(\n",
    "        [audio[\"array\"] for audio in audios],\n",
    "        sampling_rate=audios[0][\"sampling_rate\"]\n",
    "    ).input_features\n",
    "\n",
    "    # Use __call__ method (faster) - just call the tokenizer directly\n",
    "    encoded = processor.tokenizer(\n",
    "        batch[\"ref_normalized\"],\n",
    "        truncation=True,\n",
    "        padding=False  # Don't pad here, let data collator handle it\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_features\": input_features,\n",
    "        \"labels\": encoded.input_ids,  # Extract input_ids from the result\n",
    "        \"dataset\": batch[\"dataset\"]  # As metadata for evaluation\n",
    "    }\n",
    "\n",
    "\n",
    "def process_in_chunks(dataset_split, split_name, output_dir, chunk_size=10000):\n",
    "    \"\"\"Process large dataset in chunks to avoid finalization OOM\"\"\"\n",
    "    num_samples = len(dataset_split)\n",
    "    num_chunks = (num_samples + chunk_size - 1) // chunk_size\n",
    "    chunk_dir = os.path.join(output_dir, f\"{split_name}_chunks\")\n",
    "    os.makedirs(chunk_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_path = os.path.join(chunk_dir, f\"chunk_{i}\")\n",
    "\n",
    "        if os.path.exists(chunk_path):\n",
    "            print(f\"  Chunk {i + 1}/{num_chunks} exists, skipping...\")\n",
    "            continue\n",
    "\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, num_samples)\n",
    "        print(f\"  Processing chunk {i + 1}/{num_chunks} (samples {start_idx}-{end_idx})\")\n",
    "\n",
    "        chunk = dataset_split.select(range(start_idx, end_idx))\n",
    "\n",
    "        processed_chunk = chunk.map(\n",
    "            prepare_dataset_for_training,\n",
    "            batched=True,\n",
    "            batch_size=64,\n",
    "            num_proc=4,\n",
    "            keep_in_memory=False,\n",
    "            writer_batch_size=1000,\n",
    "        )\n",
    "\n",
    "        processed_chunk.save_to_disk(chunk_path)\n",
    "\n",
    "        del chunk, processed_chunk\n",
    "        gc.collect()\n",
    "\n",
    "    # Now concatenate chunks (memory-mapped, should be safe)\n",
    "    print(f\"  Concatenating {num_chunks} chunks...\")\n",
    "    chunks = [\n",
    "        load_from_disk(os.path.join(chunk_dir, f\"chunk_{i}\"))\n",
    "        for i in range(num_chunks)\n",
    "    ]\n",
    "\n",
    "    # concatenate_datasets uses memory mapping, doesn't load everything\n",
    "    final_dataset = concatenate_datasets(chunks)\n",
    "\n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "ds_dict = {}  # Should be able to load from already processed dataset\n",
    "\n",
    "# Check if fully processed\n",
    "splits = [\"validation\", \"test\"]\n",
    "all_exist = all(\n",
    "    os.path.exists(os.path.join(PROCESSED_DATASET_DIR, split))\n",
    "    for split in splits\n",
    ")\n",
    "\n",
    "if all_exist:\n",
    "    print(f\"--- Found existing processed dataset at {PROCESSED_DATASET_DIR} ---\")\n",
    "    print(\"Loading from disk to save time...\")\n",
    "    dataset = DatasetDict({\n",
    "        split: load_from_disk(os.path.join(PROCESSED_DATASET_DIR, split))\n",
    "        for split in splits\n",
    "    })\n",
    "    print(\"✓ Preprocessed dataset loaded from disk!\")\n",
    "else:\n",
    "    print(f\"--- Processed dataset not found or incomplete at {PROCESSED_DATASET_DIR} ---\")\n",
    "    print(\"Starting the heavy preprocessing (this will take a while)...\")\n",
    "    os.makedirs(PROCESSED_DATASET_DIR, exist_ok=True)\n",
    "\n",
    "    for split_name in [\"validation\", \"test\"]:\n",
    "        split_output_path = os.path.join(PROCESSED_DATASET_DIR, split_name)\n",
    "\n",
    "        if os.path.exists(split_output_path):\n",
    "            print(f\"✓ {split_name} already exists, skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {split_name} split...\")\n",
    "\n",
    "        if split_name == \"train\":\n",
    "            # Use chunked processing for large train split\n",
    "            processed_split = process_in_chunks(\n",
    "                ds_dict[split_name],\n",
    "                split_name,\n",
    "                PROCESSED_DATASET_DIR,\n",
    "                chunk_size=10000  # ~10GB chunks\n",
    "            )\n",
    "        else:\n",
    "            # Regular processing for smaller splits\n",
    "            processed_split = ds_dict[split_name].map(\n",
    "                prepare_dataset_for_training,\n",
    "                batched=True,\n",
    "                batch_size=32,\n",
    "                num_proc=8,\n",
    "                keep_in_memory=False,\n",
    "                writer_batch_size=1000,\n",
    "            )\n",
    "\n",
    "        processed_split.save_to_disk(split_output_path)\n",
    "        print(f\"✓ {split_name} saved: {len(processed_split)} samples\")\n",
    "\n",
    "        del processed_split\n",
    "        gc.collect()\n",
    "\n",
    "    # Load the complete dataset\n",
    "    dataset = DatasetDict({\n",
    "        split: load_from_disk(os.path.join(PROCESSED_DATASET_DIR, split))\n",
    "        for split in splits\n",
    "    })\n",
    "    print(\"\\n✓ All splits processed and saved!\")\n",
    "\n",
    "print(f\"✓ Validation: {len(dataset['validation'])} samples\")\n",
    "print(f\"✓ Test: {len(dataset['test'])} samples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample processed data:\")\n",
    "sample = dataset[\"test\"][0]\n",
    "print(f\"  Input features shape: {len(sample['input_features'])}\")\n",
    "print(f\"  Labels length: {len(sample['labels'])}\")\n",
    "print(f\"  First few label IDs: {sample['labels'][:10]}\")"
   ],
   "id": "516c48abee31afce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Found existing processed dataset at /root/uzbek-automatic-speech-recognition/outputs/part2/processed_uzbek_asr_dataset ---\n",
      "Loading from disk to save time...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e26cd5e4a184ed8b458cdd1b4cdf280"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessed dataset loaded from disk!\n",
      "✓ Validation: 5728 samples\n",
      "✓ Test: 6994 samples\n",
      "\n",
      "Sample processed data:\n",
      "  Input features shape: 80\n",
      "  Labels length: 29\n",
      "  First few label IDs: [50258, 50337, 50359, 50363, 35, 268, 271, 12810, 6981, 1726]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T13:22:57.688529Z",
     "start_time": "2026-02-05T13:22:57.681930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad input features\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Pad labels\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # Remove BOS token if present\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ],
   "id": "8988701ecc35207f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Base Model - Whisper Small",
   "id": "eb8a3cbf63780326"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:05:08.435171Z",
     "start_time": "2026-02-05T10:55:50.476567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scripts.whisper_utils import evaluate_by_dataset_with_trainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./base_model_eval_temp\",\n",
    "    per_device_eval_batch_size=128,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    fp16=True,\n",
    "    generation_num_beams=1,\n",
    "    dataloader_num_workers=8,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "evaluate_by_dataset_with_trainer(trainer, processor, dataset[\"test\"], \"test\")\n",
    "\n",
    "del model, trainer, processor\n",
    "gc.collect()"
   ],
   "id": "6eaf9979635da98e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "a05839a6e329993d3587600a45a814b7"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: TEST\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)             116.27%\n",
      "CER (normalized)              45.34%\n",
      "Sequence Similarity           58.79%\n",
      "WER (raw)                    118.03%\n",
      "CER (raw)                     46.87%\n",
      "Seq Similarity (raw)          57.12%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)             116.25%\n",
      "CER (normalized)              41.51%\n",
      "Sequence Similarity           67.83%\n",
      "WER (raw)                    117.47%\n",
      "CER (raw)                     42.39%\n",
      "Seq Similarity (raw)          66.52%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)             114.97%\n",
      "CER (normalized)              60.47%\n",
      "Sequence Similarity           26.50%\n",
      "WER (raw)                    116.37%\n",
      "CER (raw)                     61.80%\n",
      "Seq Similarity (raw)          23.92%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)             116.49%\n",
      "CER (normalized)              59.64%\n",
      "Sequence Similarity           23.55%\n",
      "WER (raw)                    118.99%\n",
      "CER (raw)                     62.49%\n",
      "Seq Similarity (raw)          22.13%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)             116.39%\n",
      "CER (normalized)              40.48%\n",
      "Sequence Similarity           69.19%\n",
      "WER (raw)                    118.16%\n",
      "CER (raw)                     41.77%\n",
      "Seq Similarity (raw)          67.62%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T13:27:24.121924Z",
     "start_time": "2026-02-05T13:27:24.101701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific deprecation warning\n",
    "warnings.filterwarnings(\"ignore\", message=\".*return_token_timestamps.*\")\n",
    "\n",
    "\n",
    "def test_model(model_path: str, audio_file: str):\n",
    "    \"\"\"\n",
    "    Test the fine-tuned model on a sample audio file\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to fine-tuned model\n",
    "        audio_file: Path to audio file to transcribe\n",
    "    \"\"\"\n",
    "\n",
    "    def group_words_by_sentences(chunks):\n",
    "        \"\"\"Group word-level timestamps into sentences\"\"\"\n",
    "        sentences = []\n",
    "        current_sentence = {'words': [], 'start': None, 'end': None, 'text': ''}\n",
    "\n",
    "        for chunk in chunks:\n",
    "            text = chunk['text'].strip()\n",
    "            start, end = chunk['timestamp']\n",
    "\n",
    "            # Initialize start time\n",
    "            if current_sentence['start'] is None:\n",
    "                current_sentence['start'] = start\n",
    "\n",
    "            current_sentence['words'].append(text)\n",
    "            current_sentence['end'] = end\n",
    "\n",
    "            # Check if sentence ends\n",
    "            if text.endswith('.') or text.endswith('!') or text.endswith('?'):\n",
    "                current_sentence['text'] = ' '.join(current_sentence['words'])\n",
    "                sentences.append(current_sentence.copy())\n",
    "                current_sentence = {'words': [], 'start': None, 'end': None, 'text': ''}\n",
    "\n",
    "        # Add remaining words as final sentence\n",
    "        if current_sentence['words']:\n",
    "            current_sentence['text'] = ' '.join(current_sentence['words'])\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TESTING MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    import torch\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Load model\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model_path,\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "    )\n",
    "\n",
    "    print(f\"✓ Loaded model from {model_path}\")\n",
    "    print(f\"✓ Processing: {audio_file}\")\n",
    "\n",
    "    # Transcribe\n",
    "    result = pipe(\n",
    "        audio_file,\n",
    "        language=\"uz\",\n",
    "        task=\"transcribe\",\n",
    "        return_timestamps=\"word\"\n",
    "    )\n",
    "    sentences = group_words_by_sentences(result['chunks'])\n",
    "\n",
    "    print(f\"\\nTranscription: {result['text']}\")\n",
    "    print(f\"\\nSentence-based timestamps:\")\n",
    "    for sent in sentences:\n",
    "        start_str = f\"{sent['start']:.2f}s\" if sent['start'] is not None else \"start\"\n",
    "        end_str = f\"{sent['end']:.2f}s\" if sent['end'] is not None else \"end\"\n",
    "        print(f\"[{start_str} - {end_str}]: {re.sub(r\" '\", \"'\", sent['text'])}\")\n",
    "\n",
    "    return result"
   ],
   "id": "3d4278cb48c0539b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## My Final Model",
   "id": "cdecab109ee7d658"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:31:47.125054Z",
     "start_time": "2026-02-05T11:16:29.844774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scripts.whisper_utils import load_model, evaluate_by_dataset_with_trainer\n",
    "\n",
    "FINAL_MODEL_PATH = os.path.abspath(os.path.join(OUTPUT_DIR, \"whisper-uzbek-final\"))\n",
    "\n",
    "final_model, final_trainer, final_processor = load_model(FINAL_MODEL_PATH, dataset, data_collator, eval_batch_size=192)\n",
    "evaluate_by_dataset_with_trainer(final_trainer, final_processor, dataset[\"validation\"], \"validation\")\n",
    "evaluate_by_dataset_with_trainer(final_trainer, final_processor, dataset[\"test\"], \"test\")\n",
    "\n",
    "del final_model, final_trainer, final_processor\n",
    "gc.collect()"
   ],
   "id": "58f2ada40fe9f3e2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "68f3c220b5227298aee919d6f62c4c41"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: VALIDATION\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               7.44%\n",
      "CER (normalized)               2.05%\n",
      "Sequence Similarity           95.31%\n",
      "WER (raw)                     11.35%\n",
      "CER (raw)                      2.65%\n",
      "Seq Similarity (raw)          94.30%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               7.39%\n",
      "CER (normalized)               1.65%\n",
      "Sequence Similarity           98.80%\n",
      "WER (raw)                     11.37%\n",
      "CER (raw)                      2.29%\n",
      "Seq Similarity (raw)          98.33%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              16.52%\n",
      "CER (normalized)               5.96%\n",
      "Sequence Similarity           79.90%\n",
      "WER (raw)                     23.84%\n",
      "CER (raw)                      7.19%\n",
      "Seq Similarity (raw)          75.99%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              13.96%\n",
      "CER (normalized)               5.24%\n",
      "Sequence Similarity           80.97%\n",
      "WER (raw)                     20.26%\n",
      "CER (raw)                      6.43%\n",
      "Seq Similarity (raw)          77.69%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               4.82%\n",
      "CER (normalized)               1.05%\n",
      "Sequence Similarity           99.25%\n",
      "WER (raw)                      7.70%\n",
      "CER (raw)                      1.44%\n",
      "Seq Similarity (raw)          98.96%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "7904143687e085d5e4c8ffc52d3a2c94"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: TEST\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               8.52%\n",
      "CER (normalized)               2.60%\n",
      "Sequence Similarity           94.62%\n",
      "WER (raw)                     12.86%\n",
      "CER (raw)                      3.32%\n",
      "Seq Similarity (raw)          92.92%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               8.16%\n",
      "CER (normalized)               1.98%\n",
      "Sequence Similarity           98.52%\n",
      "WER (raw)                     12.46%\n",
      "CER (raw)                      2.64%\n",
      "Seq Similarity (raw)          98.03%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              15.18%\n",
      "CER (normalized)               5.88%\n",
      "Sequence Similarity           83.65%\n",
      "WER (raw)                     21.72%\n",
      "CER (raw)                      7.05%\n",
      "Seq Similarity (raw)          77.03%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              17.19%\n",
      "CER (normalized)               6.93%\n",
      "Sequence Similarity           79.49%\n",
      "WER (raw)                     24.63%\n",
      "CER (raw)                      8.37%\n",
      "Seq Similarity (raw)          74.22%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               4.86%\n",
      "CER (normalized)               1.11%\n",
      "Sequence Similarity           99.18%\n",
      "WER (raw)                      7.75%\n",
      "CER (raw)                      1.54%\n",
      "Seq Similarity (raw)          98.85%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5744"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:44:31.483036Z",
     "start_time": "2026-02-05T11:44:25.958020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_audio = os.path.join(DATASET_DIR, \"devona_sample.wav\")\n",
    "if os.path.exists(test_audio):\n",
    "    test_model(FINAL_MODEL_PATH, test_audio)"
   ],
   "id": "f8eafbb1e9db85d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model from /root/uzbek-automatic-speech-recognition/outputs/part2/whisper-uzbek-final\n",
      "✓ Processing: ../datasets/devona_sample.wav\n",
      "\n",
      "Transcription: Deonining juda ko'p g'alati savollari bor edi. Ba'zan duch kelgan odamlardan qayerga ketyapsizlar, deb so'rar. Odamlar ham bozorga, qahvaxonaga. Uyga ketayotganlardan aytishsa, boshini ikki yoqqa silkitib, yo'q, topolmadingiz, deyar pastda yurib ketardi. Deoni odamlar bilan ish yuzasidan bo'ladigan munosabatda ham haq va huquqqa qattiq rioya etar. Bir kishining ishini bajarayotganda chin dildan ishlardi.\n",
      "\n",
      "Sentence-based timestamps:\n",
      "[0.00s - 3.42s]: Deonining juda ko'p g'alati savollari bor edi.\n",
      "[3.42s - 8.56s]: Ba'zan duch kelgan odamlardan qayerga ketyapsizlar, deb so'rar.\n",
      "[8.56s - 11.10s]: Odamlar ham bozorga, qahvaxonaga.\n",
      "[11.10s - 19.38s]: Uyga ketayotganlardan aytishsa, boshini ikki yoqqa silkitib, yo'q, topolmadingiz, deyar pastda yurib ketardi.\n",
      "[20.16s - 26.80s]: Deoni odamlar bilan ish yuzasidan bo'ladigan munosabatda ham haq va huquqqa qattiq rioya etar.\n",
      "[26.80s - 29.90s]: Bir kishining ishini bajarayotganda chin dildan ishlardi.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OvozifyLabs/whisper-small-uz-v1",
   "id": "ccee72cb62c24e10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:45:21.510214Z",
     "start_time": "2026-02-05T11:45:15.386595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=\"uz\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"OvozifyLabs/whisper-small-uz-v1\")\n",
    "\n",
    "model.generation_config.language = \"uz\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Force decoder to generate in Uzbek\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"uz\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# Set pad token to avoid attention mask warning\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "print(processor.feature_extractor)\n",
    "model"
   ],
   "id": "e06d1ad143065e45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"dither\": 0.0,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:34:09.074648Z",
     "start_time": "2026-02-02T03:25:55.924459Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "024d59a98d0ac46e54787ad1d19117bf"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: TEST\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               7.58%\n",
      "CER (normalized)               2.49%\n",
      "Sequence Similarity           94.93%\n",
      "WER (raw)                     34.50%\n",
      "CER (raw)                      6.42%\n",
      "Seq Similarity (raw)          88.22%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               6.83%\n",
      "CER (normalized)               1.68%\n",
      "Sequence Similarity           98.76%\n",
      "WER (raw)                     35.44%\n",
      "CER (raw)                      6.08%\n",
      "Seq Similarity (raw)          93.80%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              12.98%\n",
      "CER (normalized)               5.47%\n",
      "Sequence Similarity           84.89%\n",
      "WER (raw)                     28.21%\n",
      "CER (raw)                      8.02%\n",
      "Seq Similarity (raw)          72.84%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              18.14%\n",
      "CER (normalized)               7.09%\n",
      "Sequence Similarity           80.00%\n",
      "WER (raw)                     36.46%\n",
      "CER (raw)                     10.33%\n",
      "Seq Similarity (raw)          67.17%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               3.60%\n",
      "CER (normalized)               1.02%\n",
      "Sequence Similarity           99.34%\n",
      "WER (raw)                     34.37%\n",
      "CER (raw)                      5.14%\n",
      "Seq Similarity (raw)          94.59%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8,
   "source": [
    "# from scripts.whisper_utils import evaluate_by_dataset_with_trainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./base_model_eval_temp\",\n",
    "    per_device_eval_batch_size=256,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    fp16=True,\n",
    "    generation_num_beams=1,\n",
    "    dataloader_num_workers=8,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "evaluate_by_dataset_with_trainer(trainer, processor, dataset[\"test\"], \"test\")\n",
    "\n",
    "# del model, trainer, processor\n",
    "# gc.collect()"
   ],
   "id": "2117beb71fd05138"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:46:07.990627Z",
     "start_time": "2026-02-05T11:45:25.141247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_audio = os.path.join(DATASET_DIR, \"devona_sample.wav\")\n",
    "if os.path.exists(test_audio):\n",
    "    test_model(\"OvozifyLabs/whisper-small-uz-v1\", test_audio)"
   ],
   "id": "cac745d39d1e7e89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model from OvozifyLabs/whisper-small-uz-v1\n",
      "✓ Processing: ../datasets/devona_sample.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription:  devonining juda ko‘p g‘alati savollari bor edi. ba’zan duch kelgan odamlardan qayerga ketyapsizlar, deb so‘rar. odamlar ham bozorga, qahvaxonaga, uyga ketayotganlardan aytishsa, boshini ikki yoqqa silkitib, yo‘q, topolmadingiz, der, asta yurib ketardi. devona odamlar bilan ish yuzasidan bo‘ladigan munosabatda ham haq va huquqqa qattiq rioya etar. bir kishining ishini bajarayotganda chin dildan ishlardi.\n",
      "\n",
      "Sentence-based timestamps:\n",
      "[0.00s - 3.52s]: devonining juda ko‘p g‘alati savollari bor edi.\n",
      "[3.52s - 8.60s]: ba’zan duch kelgan odamlardan qayerga ketyapsizlar, deb so‘rar.\n",
      "[8.60s - 20.08s]: odamlar ham bozorga, qahvaxonaga, uyga ketayotganlardan aytishsa, boshini ikki yoqqa silkitib, yo‘q, topolmadingiz, der, asta yurib ketardi.\n",
      "[20.08s - 26.90s]: devona odamlar bilan ish yuzasidan bo‘ladigan munosabatda ham haq va huquqqa qattiq rioya etar.\n",
      "[26.90s - 29.90s]: bir kishining ishini bajarayotganda chin dildan ishlardi.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## islomov/rubaistt_v2_medium",
   "id": "d8a7908a65d4e550"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T13:23:00.365065Z",
     "start_time": "2026-02-05T13:22:57.925642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(DATASET_PATH, index_col=\"id\", low_memory=False)\n",
    "\n",
    "# Shuffle dataset\n",
    "df = df.sample(frac=1, random_state=SEED)\n",
    "\n",
    "# Create full absolute path to audio\n",
    "df[\"path\"] = df.apply(\n",
    "    lambda row: os.path.abspath(\n",
    "        os.path.join(DATASET_DIR, row[\"dataset\"], \"sampled_audio\", row[\"path\"])\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "cols = [\"path\", \"type\", \"dataset\", \"duration\", \"ref_normalized\"]\n",
    "df = df[cols]\n",
    "\n",
    "# Make sure ref_normalized is never NaN\n",
    "# There was an exception thrown while processing dataset\n",
    "none_mask = df[\"ref_normalized\"].isna() | df[\"ref_normalized\"].isnull()\n",
    "df.loc[none_mask, \"ref_normalized\"] = \"\"\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"  Total duration: {df['duration'].sum() / 3600:.2f} hours\")\n",
    "print(f\"  Avg duration: {df['duration'].mean():.2f} seconds\")\n",
    "print(f\"  By Dataset:\")\n",
    "print((df.groupby([\"dataset\", \"type\"])[\"duration\"].sum() / 3600))\n",
    "print(f\"Total training samples: {len(df[df[\"type\"] == \"train\"]):,}\")\n",
    "print(f\"Total validation samples: {len(df[df[\"type\"] == \"validation\"]):,}\")\n",
    "print(f\"Total test samples: {len(df[df[\"type\"] == \"test\"]):,}\")\n",
    "\n",
    "df.to_csv(os.path.join(DATASET_DIR, \"combined_dataset_part2.csv\"), index_label=\"id\")\n",
    "df"
   ],
   "id": "127af2004314ec3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "  Total duration: 161.00 hours\n",
      "  Avg duration: 7.31 seconds\n",
      "  By Dataset:\n",
      "dataset        type      \n",
      "common_voice   test           2.751890\n",
      "               validation     1.328719\n",
      "feruza_speech  train          3.217020\n",
      "it             test           0.737151\n",
      "               train          7.940193\n",
      "               validation     0.735187\n",
      "news           test           1.726184\n",
      "               train         51.826843\n",
      "               validation     1.106672\n",
      "uzbek_voice    test           7.160320\n",
      "               train         75.527433\n",
      "               validation     6.939970\n",
      "Name: duration, dtype: float64\n",
      "Total training samples: 66,545\n",
      "Total validation samples: 5,728\n",
      "Total test samples: 6,994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                    path        type  \\\n",
       "id                                                                     \n",
       "69899  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "64879  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "6928   /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "24018  /root/uzbek-automatic-speech-recognition/datas...  validation   \n",
       "36042  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "...                                                  ...         ...   \n",
       "20665  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "35905  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "24573  /root/uzbek-automatic-speech-recognition/datas...  validation   \n",
       "44775  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "15562  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "\n",
       "           dataset  duration  \\\n",
       "id                             \n",
       "69899         news    8.2418   \n",
       "64879         news    5.7762   \n",
       "6928   uzbek_voice    5.6160   \n",
       "24018  uzbek_voice    6.0840   \n",
       "36042  uzbek_voice    6.1136   \n",
       "...            ...       ...   \n",
       "20665  uzbek_voice    7.1280   \n",
       "35905  uzbek_voice    5.1304   \n",
       "24573  uzbek_voice    3.8160   \n",
       "44775  uzbek_voice    5.0760   \n",
       "15562  uzbek_voice    5.2560   \n",
       "\n",
       "                                          ref_normalized  \n",
       "id                                                        \n",
       "69899  Bir vaqtda sodir bo'ldi. Nega aynan shunday za...  \n",
       "64879  Rahmat berib o'tgan fikrlaringiz va ma'lumotla...  \n",
       "6928   O'yinchoqni olamiz, bolani qorni bilan fitbol ...  \n",
       "24018  E'londa kriptovalyutadagi narx vaqtincha belgi...  \n",
       "36042  Mazkur anjumanda iqlim bo'yicha yangi xalqaro ...  \n",
       "...                                                  ...  \n",
       "20665  Organizmda yo'qotilgan unsurning o'rnini to'ld...  \n",
       "35905  To'fon pasayganiga qaramay, xavf hali ham saql...  \n",
       "24573     Bu haqda hokimlik axborot xizmati xabar berdi.  \n",
       "44775  Shu yerda qishloq xo'jaligida amalga oshirilad...  \n",
       "15562  Balki bir muammomiz ikkita va undan ko'pga ort...  \n",
       "\n",
       "[79267 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>type</th>\n",
       "      <th>dataset</th>\n",
       "      <th>duration</th>\n",
       "      <th>ref_normalized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69899</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>news</td>\n",
       "      <td>8.2418</td>\n",
       "      <td>Bir vaqtda sodir bo'ldi. Nega aynan shunday za...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64879</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>news</td>\n",
       "      <td>5.7762</td>\n",
       "      <td>Rahmat berib o'tgan fikrlaringiz va ma'lumotla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6928</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>5.6160</td>\n",
       "      <td>O'yinchoqni olamiz, bolani qorni bilan fitbol ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24018</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>validation</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>6.0840</td>\n",
       "      <td>E'londa kriptovalyutadagi narx vaqtincha belgi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36042</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>6.1136</td>\n",
       "      <td>Mazkur anjumanda iqlim bo'yicha yangi xalqaro ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20665</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>7.1280</td>\n",
       "      <td>Organizmda yo'qotilgan unsurning o'rnini to'ld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35905</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>5.1304</td>\n",
       "      <td>To'fon pasayganiga qaramay, xavf hali ham saql...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24573</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>validation</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>3.8160</td>\n",
       "      <td>Bu haqda hokimlik axborot xizmati xabar berdi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44775</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>5.0760</td>\n",
       "      <td>Shu yerda qishloq xo'jaligida amalga oshirilad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15562</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>5.2560</td>\n",
       "      <td>Balki bir muammomiz ikkita va undan ko'pga ort...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79267 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T13:23:01.297524Z",
     "start_time": "2026-02-05T13:23:01.235757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Audio, Dataset\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "ds_dict = DatasetDict({\n",
    "    \"test\": Dataset.from_pandas(df[df[\"type\"] == \"test\"]),\n",
    "})\n",
    "ds_dict = ds_dict.remove_columns([\"type\", \"duration\"])\n",
    "\n",
    "# Cast the path column to Audio\n",
    "ds_dict = ds_dict.cast_column(\"path\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# Rename columns for clarity\n",
    "ds_dict = ds_dict.rename_column(\"path\", \"audio\")\n",
    "\n",
    "ds_dict"
   ],
   "id": "718044f6a4787fa0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'dataset', 'ref_normalized', 'id'],\n",
       "        num_rows: 6994\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:47:42.861260Z",
     "start_time": "2026-02-05T11:47:30.282613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"uz\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"islomov/rubaistt_v2_medium\")\n",
    "\n",
    "model.generation_config.language = \"uz\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Force decoder to generate in Uzbek\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"uz\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# Set pad token to avoid attention mask warning\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "print(processor.feature_extractor)\n",
    "model"
   ],
   "id": "39ea4372587c16d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"dither\": 0.0,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T13:23:03.597019Z",
     "start_time": "2026-02-05T13:23:03.515613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import os\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "NEW_PROCESSED_DATASET_DIR = os.path.join(\"../outputs/whisper_medium/processed_dataset\")\n",
    "\n",
    "\n",
    "def prepare_dataset_for_training(batch):\n",
    "    \"\"\"Preprocess a batch for training\"\"\"\n",
    "    # Load and process audio from the 'audio' column\n",
    "    audios = batch[\"audio\"]\n",
    "\n",
    "    # Compute log-Mel input features\n",
    "    input_features = processor.feature_extractor(\n",
    "        [audio[\"array\"] for audio in audios],\n",
    "        sampling_rate=audios[0][\"sampling_rate\"]\n",
    "    ).input_features\n",
    "\n",
    "    # Use __call__ method (faster) - just call the tokenizer directly\n",
    "    encoded = processor.tokenizer(\n",
    "        batch[\"ref_normalized\"],\n",
    "        truncation=True,\n",
    "        padding=False  # Don't pad here, let data collator handle it\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_features\": input_features,\n",
    "        \"labels\": encoded.input_ids,  # Extract input_ids from the result\n",
    "        \"dataset\": batch[\"dataset\"]  # As metadata for evaluation\n",
    "    }\n",
    "\n",
    "\n",
    "def process_in_chunks(dataset_split, split_name, output_dir, chunk_size=10000):\n",
    "    \"\"\"Process large dataset in chunks to avoid finalization OOM\"\"\"\n",
    "    num_samples = len(dataset_split)\n",
    "    num_chunks = (num_samples + chunk_size - 1) // chunk_size\n",
    "    chunk_dir = os.path.join(output_dir, f\"{split_name}_chunks\")\n",
    "    os.makedirs(chunk_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_path = os.path.join(chunk_dir, f\"chunk_{i}\")\n",
    "\n",
    "        if os.path.exists(chunk_path):\n",
    "            print(f\"  Chunk {i + 1}/{num_chunks} exists, skipping...\")\n",
    "            continue\n",
    "\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, num_samples)\n",
    "        print(f\"  Processing chunk {i + 1}/{num_chunks} (samples {start_idx}-{end_idx})\")\n",
    "\n",
    "        chunk = dataset_split.select(range(start_idx, end_idx))\n",
    "\n",
    "        processed_chunk = chunk.map(\n",
    "            prepare_dataset_for_training,\n",
    "            batched=True,\n",
    "            batch_size=64,\n",
    "            num_proc=4,\n",
    "            keep_in_memory=False,\n",
    "            writer_batch_size=1000,\n",
    "        )\n",
    "\n",
    "        processed_chunk.save_to_disk(chunk_path)\n",
    "\n",
    "        del chunk, processed_chunk\n",
    "        gc.collect()\n",
    "\n",
    "    # Now concatenate chunks (memory-mapped, should be safe)\n",
    "    print(f\"  Concatenating {num_chunks} chunks...\")\n",
    "    chunks = [\n",
    "        load_from_disk(os.path.join(chunk_dir, f\"chunk_{i}\"))\n",
    "        for i in range(num_chunks)\n",
    "    ]\n",
    "\n",
    "    # concatenate_datasets uses memory mapping, doesn't load everything\n",
    "    final_dataset = concatenate_datasets(chunks)\n",
    "\n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "# Check if fully processed\n",
    "splits = [\"test\"]\n",
    "all_exist = all(\n",
    "    os.path.exists(os.path.join(NEW_PROCESSED_DATASET_DIR, split))\n",
    "    for split in splits\n",
    ")\n",
    "\n",
    "if all_exist:\n",
    "    print(f\"--- Found existing processed dataset at {NEW_PROCESSED_DATASET_DIR} ---\")\n",
    "    print(\"Loading from disk to save time...\")\n",
    "    dataset = DatasetDict({\n",
    "        split: load_from_disk(os.path.join(NEW_PROCESSED_DATASET_DIR, split))\n",
    "        for split in splits\n",
    "    })\n",
    "    print(\"✓ Preprocessed dataset loaded from disk!\")\n",
    "else:\n",
    "    print(f\"--- Processed dataset not found or incomplete at {NEW_PROCESSED_DATASET_DIR} ---\")\n",
    "    print(\"Starting the heavy preprocessing (this will take a while)...\")\n",
    "    os.makedirs(NEW_PROCESSED_DATASET_DIR, exist_ok=True)\n",
    "\n",
    "    for split_name in [\"test\"]:\n",
    "        split_output_path = os.path.join(NEW_PROCESSED_DATASET_DIR, split_name)\n",
    "\n",
    "        if os.path.exists(split_output_path):\n",
    "            print(f\"✓ {split_name} already exists, skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {split_name} split...\")\n",
    "\n",
    "        if split_name == \"train\":\n",
    "            # Use chunked processing for large train split\n",
    "            processed_split = process_in_chunks(\n",
    "                ds_dict[split_name],\n",
    "                split_name,\n",
    "                NEW_PROCESSED_DATASET_DIR,\n",
    "                chunk_size=10000  # ~10GB chunks\n",
    "            )\n",
    "        else:\n",
    "            # Regular processing for smaller splits\n",
    "            processed_split = ds_dict[split_name].map(\n",
    "                prepare_dataset_for_training,\n",
    "                batched=True,\n",
    "                batch_size=32,\n",
    "                num_proc=8,\n",
    "                keep_in_memory=False,\n",
    "                writer_batch_size=1000,\n",
    "            )\n",
    "\n",
    "        processed_split.save_to_disk(split_output_path)\n",
    "        print(f\"✓ {split_name} saved: {len(processed_split)} samples\")\n",
    "\n",
    "        del processed_split\n",
    "        gc.collect()\n",
    "\n",
    "    # Load the complete dataset\n",
    "    dataset = DatasetDict({\n",
    "        split: load_from_disk(os.path.join(NEW_PROCESSED_DATASET_DIR, split))\n",
    "        for split in splits\n",
    "    })\n",
    "    print(\"\\n✓ All splits processed and saved!\")"
   ],
   "id": "3e8d181a918217a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Found existing processed dataset at ../outputs/whisper_medium/processed_dataset ---\n",
      "Loading from disk to save time...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c24cf2889394f9bb76f8a15fe103d17"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessed dataset loaded from disk!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T02:55:39.088369Z",
     "start_time": "2026-02-02T02:34:44.620574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./base_model_eval_temp\",\n",
    "    per_device_eval_batch_size=128,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    fp16=True,\n",
    "    generation_num_beams=1,\n",
    "    dataloader_num_workers=8,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "evaluate_by_dataset_with_trainer(trainer, processor, dataset[\"test\"], \"test\")\n",
    "\n",
    "del model, trainer, processor\n",
    "gc.collect()"
   ],
   "id": "fe8cab7685bf6d82",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "fd16a9d38e26cd1d8177044d3dac7a68"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: TEST\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               7.33%\n",
      "CER (normalized)               1.76%\n",
      "Sequence Similarity           96.78%\n",
      "WER (raw)                     37.40%\n",
      "CER (raw)                      8.16%\n",
      "Seq Similarity (raw)          89.26%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               6.33%\n",
      "CER (normalized)               1.16%\n",
      "Sequence Similarity           99.14%\n",
      "WER (raw)                     31.73%\n",
      "CER (raw)                      4.84%\n",
      "Seq Similarity (raw)          96.29%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               9.33%\n",
      "CER (normalized)               2.84%\n",
      "Sequence Similarity           91.24%\n",
      "WER (raw)                     26.07%\n",
      "CER (raw)                      6.26%\n",
      "Seq Similarity (raw)          76.07%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              12.84%\n",
      "CER (normalized)               4.28%\n",
      "Sequence Similarity           88.27%\n",
      "WER (raw)                     33.03%\n",
      "CER (raw)                      9.59%\n",
      "Seq Similarity (raw)          71.70%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               5.59%\n",
      "CER (normalized)               1.07%\n",
      "Sequence Similarity           99.23%\n",
      "WER (raw)                     42.68%\n",
      "CER (raw)                      9.15%\n",
      "Seq Similarity (raw)          93.89%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:50:26.954652Z",
     "start_time": "2026-02-05T11:48:09.907145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_audio = os.path.join(DATASET_DIR, \"devona_sample.wav\")\n",
    "if os.path.exists(test_audio):\n",
    "    test_model(\"islomov/rubaistt_v2_medium\", test_audio)"
   ],
   "id": "a6c26402e4427e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model from islomov/rubaistt_v2_medium\n",
      "✓ Processing: ../datasets/devona_sample.wav\n",
      "\n",
      "Transcription: devonaning juda ko'p g'alati savollari bor edi. ba'zan duch kelgan odamlardan qayerga ketyapsizlar? deb so'rar. odamlar ham bozorga, qahvaxonaga, uyga ketayotganlardan aytishsa boshini ikki yoqqa silkitib, yo'q, topolmadingiz, der, vasta yurib ketardi.devona odamlar bilan ish yuzasidan bo'ladigan munosabatda ham haq va huquqqa qattiq rioya etar. bir kishining ishini bajarayotganda chin dildan ishlardi.\n",
      "\n",
      "Sentence-based timestamps:\n",
      "[0.00s - 3.58s]: devonaning juda ko'p g'alati savollari bor edi.\n",
      "[3.58s - 7.68s]: ba'zan duch kelgan odamlardan qayerga ketyapsizlar?\n",
      "[7.68s - 8.54s]: deb so'rar.\n",
      "[8.54s - 20.04s]: odamlar ham bozorga, qahvaxonaga, uyga ketayotganlardan aytishsa boshini ikki yoqqa silkitib, yo'q, topolmadingiz, der, vasta yurib ketardi.\n",
      "[19.52s - 26.78s]: devona odamlar bilan ish yuzasidan bo'ladigan munosabatda ham haq va huquqqa qattiq rioya etar.\n",
      "[26.78s - 29.98s]: bir kishining ishini bajarayotganda chin dildan ishlardi.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Kotib/uzbek_stt_v1 (Whisper Medium)",
   "id": "dda01efb8b4257fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:19:07.351793Z",
     "start_time": "2026-02-02T02:57:37.131936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"uz\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"Kotib/uzbek_stt_v1\")\n",
    "\n",
    "model.generation_config.language = \"uz\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Force decoder to generate in Uzbek\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"uz\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# Set pad token to avoid attention mask warning\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./base_model_eval_temp\",\n",
    "    per_device_eval_batch_size=128,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    fp16=True,\n",
    "    generation_num_beams=1,\n",
    "    dataloader_num_workers=8,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "evaluate_by_dataset_with_trainer(trainer, processor, dataset[\"test\"], \"test\")\n",
    "\n",
    "del model, trainer, processor\n",
    "gc.collect()"
   ],
   "id": "f80cc819a58eed18",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "7b58c3745d5261a8ac9f9c2121d3a255"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: TEST\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               5.62%\n",
      "CER (normalized)               1.37%\n",
      "Sequence Similarity           96.93%\n",
      "WER (raw)                     28.01%\n",
      "CER (raw)                      6.77%\n",
      "Seq Similarity (raw)          89.89%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               5.28%\n",
      "CER (normalized)               0.99%\n",
      "Sequence Similarity           99.28%\n",
      "WER (raw)                     23.28%\n",
      "CER (raw)                      3.67%\n",
      "Seq Similarity (raw)          96.91%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               8.78%\n",
      "CER (normalized)               2.65%\n",
      "Sequence Similarity           91.88%\n",
      "WER (raw)                     25.18%\n",
      "CER (raw)                      6.03%\n",
      "Seq Similarity (raw)          76.91%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              12.55%\n",
      "CER (normalized)               4.21%\n",
      "Sequence Similarity           87.38%\n",
      "WER (raw)                     32.97%\n",
      "CER (raw)                      9.49%\n",
      "Seq Similarity (raw)          70.39%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               3.01%\n",
      "CER (normalized)               0.48%\n",
      "Sequence Similarity           99.65%\n",
      "WER (raw)                     28.56%\n",
      "CER (raw)                      7.13%\n",
      "Seq Similarity (raw)          95.05%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:53:57.416749Z",
     "start_time": "2026-02-05T11:51:43.418998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_audio = os.path.join(DATASET_DIR, \"devona_sample.wav\")\n",
    "if os.path.exists(test_audio):\n",
    "    test_model(\"Kotib/uzbek_stt_v1\", test_audio)"
   ],
   "id": "9a7c7eca047242ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model from Kotib/uzbek_stt_v1\n",
      "✓ Processing: ../datasets/devona_sample.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription: devonaning juda ko'p g'alati savollari bor edi. ba'zan duch kelgan odamlardan qayerga ketyapsizlar? deb so'rar. odamlar ham bozorga, qahvaxonaga, uyga ketayotganlarni aytishsa boshini ikki yoqqa silkitib, yo'q, topa olmadingiz, der, asta yurib ketardi. devona odamlar bilan ish yuzasidan bo'ladigan munosabatda ham haq va huquqqa qattiq rioya etar. bir kishining ishini bajarayotganda chin dildan ishlardi.\n",
      "\n",
      "Sentence-based timestamps:\n",
      "[0.00s - 3.62s]: devonaning juda ko'p g'alati savollari bor edi.\n",
      "[3.62s - 7.68s]: ba'zan duch kelgan odamlardan qayerga ketyapsizlar?\n",
      "[7.68s - 8.56s]: deb so'rar.\n",
      "[8.56s - 20.20s]: odamlar ham bozorga, qahvaxonaga, uyga ketayotganlarni aytishsa boshini ikki yoqqa silkitib, yo'q, topa olmadingiz, der, asta yurib ketardi.\n",
      "[20.20s - 26.88s]: devona odamlar bilan ish yuzasidan bo'ladigan munosabatda ham haq va huquqqa qattiq rioya etar.\n",
      "[26.88s - 29.90s]: bir kishining ishini bajarayotganda chin dildan ishlardi.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## aisha-org/Whisper-Uzbek",
   "id": "2d9b7795fa06623d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T12:27:29.611704Z",
     "start_time": "2026-02-05T12:06:05.753737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"uz\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"aisha-org/Whisper-Uzbek\")\n",
    "\n",
    "model.generation_config.language = \"uz\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Force decoder to generate in Uzbek\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"uz\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# Set pad token to avoid attention mask warning\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./base_model_eval_temp\",\n",
    "    per_device_eval_batch_size=128,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    fp16=True,\n",
    "    generation_num_beams=1,\n",
    "    dataloader_num_workers=8,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "evaluate_by_dataset_with_trainer(trainer, processor, dataset[\"test\"], \"test\")\n",
    "\n",
    "del model, trainer, processor\n",
    "gc.collect()"
   ],
   "id": "682aacff7188357d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "76768f5606e37887a5e9f49bb51d6e31"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: TEST\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              17.28%\n",
      "CER (normalized)               5.49%\n",
      "Sequence Similarity           90.40%\n",
      "WER (raw)                     40.11%\n",
      "CER (raw)                     11.43%\n",
      "Seq Similarity (raw)          83.67%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               8.76%\n",
      "CER (normalized)               1.93%\n",
      "Sequence Similarity           98.59%\n",
      "WER (raw)                     28.40%\n",
      "CER (raw)                      5.14%\n",
      "Seq Similarity (raw)          95.82%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              39.98%\n",
      "CER (normalized)              15.28%\n",
      "Sequence Similarity           61.59%\n",
      "WER (raw)                     58.48%\n",
      "CER (raw)                     20.16%\n",
      "Seq Similarity (raw)          48.47%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              40.58%\n",
      "CER (normalized)              17.31%\n",
      "Sequence Similarity           64.61%\n",
      "WER (raw)                     59.10%\n",
      "CER (raw)                     23.28%\n",
      "Seq Similarity (raw)          49.21%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               9.57%\n",
      "CER (normalized)               1.95%\n",
      "Sequence Similarity           98.56%\n",
      "WER (raw)                     35.55%\n",
      "CER (raw)                      8.92%\n",
      "Seq Similarity (raw)          93.95%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11230"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T13:19:54.731856Z",
     "start_time": "2026-02-05T13:19:26.211586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_audio = os.path.join(DATASET_DIR, \"devona_sample.wav\")\n",
    "if os.path.exists(test_audio):\n",
    "    test_model(\"aisha-org/Whisper-Uzbek\", test_audio)"
   ],
   "id": "4eeb612a14a03c26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a876f744-f755-4528-9fa1-1b637ca7e2c9)')' thrown while requesting HEAD https://huggingface.co/aisha-org/Whisper-Uzbek/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model from aisha-org/Whisper-Uzbek\n",
      "✓ Processing: ../datasets/devona_sample.wav\n",
      "\n",
      "Transcription: Devonaning juda koʻp gʻalati savollari bor edi. Baʼzan duch kelgan odamlardan «qaerga ketayapsizlar?» deb soʻrar, odamlar ham bozorga, qahvaxonaga, uyga ketayotganlaridan aytishsa, boshini ikki yoqqa silkitib «yoʻq, topolmadingiz!», der, ostida yurib ketardi.Devona odamlar bilan ish yuzasidan bo‘ladigan munosabatda ham haq va huquqqa qattiq rioya etar, bir kishining ishini bajarayotganda chin dildan ishlardi.\n",
      "\n",
      "Sentence-based timestamps:\n",
      "[0.00s - 3.62s]: Devonaning juda koʻp gʻalati savollari bor edi.\n",
      "[3.62s - 20.06s]: Baʼzan duch kelgan odamlardan «qaerga ketayapsizlar ?» deb soʻrar, odamlar ham bozorga, qahvaxonaga, uyga ketayotganlaridan aytishsa, boshini ikki yoqqa silkitib «yoʻq, topolmadingiz !», der, ostida yurib ketardi.\n",
      "[19.64s - 29.96s]: Devona odamlar bilan ish yuzasidan bo‘ladigan munosabatda ham haq va huquqqa qattiq rioya etar, bir kishining ishini bajarayotganda chin dildan ishlardi.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## AbdulxoliqMirzaev/whisper-uz-medium",
   "id": "1fbac19f4219b876"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T13:04:07.822244Z",
     "start_time": "2026-02-05T12:42:27.754421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"uz\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"AbdulxoliqMirzaev/whisper-uz-medium\")\n",
    "\n",
    "model.generation_config.language = \"uz\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Force decoder to generate in Uzbek\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"uz\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# Set pad token to avoid attention mask warning\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./base_model_eval_temp\",\n",
    "    per_device_eval_batch_size=192,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    fp16=True,\n",
    "    generation_num_beams=1,\n",
    "    dataloader_num_workers=8,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "evaluate_by_dataset_with_trainer(trainer, processor, dataset[\"test\"], \"test\")\n",
    "\n",
    "del model, trainer, processor\n",
    "gc.collect()"
   ],
   "id": "815f0d4e12da848c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc0a2a961d6846d088a02cf90158156b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d22e629c807454aa733fdce29442159"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01562690a3964269a334fb5a4199848f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "3fedb1e74585993542bf3363e51ae6c0"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: TEST\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              23.36%\n",
      "CER (normalized)               6.53%\n",
      "Sequence Similarity           88.90%\n",
      "WER (raw)                     30.14%\n",
      "CER (raw)                      9.82%\n",
      "Seq Similarity (raw)          85.38%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              17.03%\n",
      "CER (normalized)               3.51%\n",
      "Sequence Similarity           97.38%\n",
      "WER (raw)                     22.72%\n",
      "CER (raw)                      4.60%\n",
      "Seq Similarity (raw)          96.59%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              49.46%\n",
      "CER (normalized)              17.87%\n",
      "Sequence Similarity           57.87%\n",
      "WER (raw)                     59.63%\n",
      "CER (raw)                     20.48%\n",
      "Seq Similarity (raw)          46.44%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              47.52%\n",
      "CER (normalized)              18.01%\n",
      "Sequence Similarity           60.21%\n",
      "WER (raw)                     58.23%\n",
      "CER (raw)                     21.66%\n",
      "Seq Similarity (raw)          52.57%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              14.09%\n",
      "CER (normalized)               2.72%\n",
      "Sequence Similarity           98.02%\n",
      "WER (raw)                     19.27%\n",
      "CER (raw)                      6.69%\n",
      "Seq Similarity (raw)          95.80%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11220"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T13:30:19.271835Z",
     "start_time": "2026-02-05T13:29:16.217524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"uz\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"AbdulxoliqMirzaev/whisper-uz-medium\")\n",
    "\n",
    "model.generation_config.language = \"uz\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Force decoder to generate in Uzbek\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"uz\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# Set pad token to avoid attention mask warning\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "test_audio = os.path.join(DATASET_DIR, \"devona_sample.wav\")\n",
    "if os.path.exists(test_audio):\n",
    "    test_model(\"AbdulxoliqMirzaev/whisper-uz-medium\", test_audio)"
   ],
   "id": "c3d0c0329babb4cd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 310b009a-e6f8-4a67-b979-066b7a6e1f9b)')' thrown while requesting HEAD https://huggingface.co/openai/whisper-medium/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d60d0454-8251-4114-8451-0648f96bb7fb)')' thrown while requesting HEAD https://huggingface.co/openai/whisper-medium/resolve/main/processor_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 50045734-e638-48db-9566-a3613c326e3a)')' thrown while requesting HEAD https://huggingface.co/openai/whisper-medium/resolve/main/chat_template.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d755d1cf-a8a5-4eea-9203-10e577394079)')' thrown while requesting HEAD https://huggingface.co/openai/whisper-medium/resolve/main/audio_tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING MODEL\n",
      "==================================================\n",
      "✓ Loaded model object\n",
      "✓ Processing: ../datasets/devona_sample.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription: Devonaning juda ko'p g'alati savollari bor edi, ba'zan duch kelgan odamlardan qayerga ketayapsizlar, deb so'rar, odamlar ham bozorga, qahvaxonaga, uyga ketayotganlarda aytishsa boshini ikki yoqqasil ketib, yo'q, topolmadingiz, der, vasta yurib ketardi.Devona odamlar bilan ish yuzasidan bo'latgan munosabatda ham haq va huquqqa qattiq rioya etar, bir kishining ishini bajarayotgandi chindildan ishlardi.\n",
      "\n",
      "Sentence-based timestamps:\n",
      "[0.00s - 19.48s]: Devonaning juda ko 'p g 'alati savollari bor edi, ba 'zan duch kelgan odamlardan qayerga ketayapsizlar, deb so 'rar, odamlar ham bozorga, qahvaxonaga, uyga ketayotganlarda aytishsa boshini ikki yoqqasil ketib, yo 'q, topolmadingiz, der, vasta yurib ketardi.\n",
      "[19.52s - 29.98s]: Devona odamlar bilan ish yuzasidan bo 'latgan munosabatda ham haq va huquqqa qattiq rioya etar, bir kishining ishini bajarayotgandi chindildan ishlardi.\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
