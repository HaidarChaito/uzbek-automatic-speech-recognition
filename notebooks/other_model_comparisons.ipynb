{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-02T03:24:25.160260Z",
     "start_time": "2026-02-02T03:24:25.154314Z"
    }
   },
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to Python path\n",
    "parent_dir = Path('.').absolute().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:24:41.957749Z",
     "start_time": "2026-02-02T03:24:36.375585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "DATASET_DIR = \"../datasets\"\n",
    "DATASET_PATH = os.path.join(DATASET_DIR, \"combined__case_sensitive_part2.csv\")\n",
    "SEED = 137\n",
    "\n",
    "OUTPUT_DIR = \"../outputs/part2\"\n",
    "PROCESSED_DATASET_DIR = os.path.abspath(os.path.join(OUTPUT_DIR, \"processed_uzbek_asr_dataset\"))\n",
    "\n",
    "MODEL_NAME = \"openai/whisper-small\""
   ],
   "id": "fcb9432595da9741",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:24:49.868218Z",
     "start_time": "2026-02-02T03:24:47.459804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=\"uz\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model.generation_config.language = \"uz\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Force decoder to generate in Uzbek\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"uz\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# Set pad token to avoid attention mask warning\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "print(processor.feature_extractor)\n",
    "model"
   ],
   "id": "650a28c5e837a62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"dither\": 0.0,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:24:52.985378Z",
     "start_time": "2026-02-02T03:24:52.438079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import os\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "\n",
    "def prepare_dataset_for_training(batch):\n",
    "    \"\"\"Preprocess a batch for training\"\"\"\n",
    "    # Load and process audio from the 'audio' column\n",
    "    audios = batch[\"audio\"]\n",
    "\n",
    "    # Compute log-Mel input features\n",
    "    input_features = processor.feature_extractor(\n",
    "        [audio[\"array\"] for audio in audios],\n",
    "        sampling_rate=audios[0][\"sampling_rate\"]\n",
    "    ).input_features\n",
    "\n",
    "    # Use __call__ method (faster) - just call the tokenizer directly\n",
    "    encoded = processor.tokenizer(\n",
    "        batch[\"ref_normalized\"],\n",
    "        truncation=True,\n",
    "        padding=False  # Don't pad here, let data collator handle it\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_features\": input_features,\n",
    "        \"labels\": encoded.input_ids,  # Extract input_ids from the result\n",
    "        \"dataset\": batch[\"dataset\"]  # As metadata for evaluation\n",
    "    }\n",
    "\n",
    "\n",
    "def process_in_chunks(dataset_split, split_name, output_dir, chunk_size=10000):\n",
    "    \"\"\"Process large dataset in chunks to avoid finalization OOM\"\"\"\n",
    "    num_samples = len(dataset_split)\n",
    "    num_chunks = (num_samples + chunk_size - 1) // chunk_size\n",
    "    chunk_dir = os.path.join(output_dir, f\"{split_name}_chunks\")\n",
    "    os.makedirs(chunk_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_path = os.path.join(chunk_dir, f\"chunk_{i}\")\n",
    "\n",
    "        if os.path.exists(chunk_path):\n",
    "            print(f\"  Chunk {i + 1}/{num_chunks} exists, skipping...\")\n",
    "            continue\n",
    "\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, num_samples)\n",
    "        print(f\"  Processing chunk {i + 1}/{num_chunks} (samples {start_idx}-{end_idx})\")\n",
    "\n",
    "        chunk = dataset_split.select(range(start_idx, end_idx))\n",
    "\n",
    "        processed_chunk = chunk.map(\n",
    "            prepare_dataset_for_training,\n",
    "            batched=True,\n",
    "            batch_size=64,\n",
    "            num_proc=4,\n",
    "            keep_in_memory=False,\n",
    "            writer_batch_size=1000,\n",
    "        )\n",
    "\n",
    "        processed_chunk.save_to_disk(chunk_path)\n",
    "\n",
    "        del chunk, processed_chunk\n",
    "        gc.collect()\n",
    "\n",
    "    # Now concatenate chunks (memory-mapped, should be safe)\n",
    "    print(f\"  Concatenating {num_chunks} chunks...\")\n",
    "    chunks = [\n",
    "        load_from_disk(os.path.join(chunk_dir, f\"chunk_{i}\"))\n",
    "        for i in range(num_chunks)\n",
    "    ]\n",
    "\n",
    "    # concatenate_datasets uses memory mapping, doesn't load everything\n",
    "    final_dataset = concatenate_datasets(chunks)\n",
    "\n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "ds_dict = {}  # Should be able to load from already processed dataset\n",
    "\n",
    "# Check if fully processed\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "all_exist = all(\n",
    "    os.path.exists(os.path.join(PROCESSED_DATASET_DIR, split))\n",
    "    for split in splits\n",
    ")\n",
    "\n",
    "if all_exist:\n",
    "    print(f\"--- Found existing processed dataset at {PROCESSED_DATASET_DIR} ---\")\n",
    "    print(\"Loading from disk to save time...\")\n",
    "    dataset = DatasetDict({\n",
    "        split: load_from_disk(os.path.join(PROCESSED_DATASET_DIR, split))\n",
    "        for split in splits\n",
    "    })\n",
    "    print(\"✓ Preprocessed dataset loaded from disk!\")\n",
    "else:\n",
    "    print(f\"--- Processed dataset not found or incomplete at {PROCESSED_DATASET_DIR} ---\")\n",
    "    print(\"Starting the heavy preprocessing (this will take a while)...\")\n",
    "    os.makedirs(PROCESSED_DATASET_DIR, exist_ok=True)\n",
    "\n",
    "    for split_name in [\"train\", \"validation\", \"test\"]:\n",
    "        split_output_path = os.path.join(PROCESSED_DATASET_DIR, split_name)\n",
    "\n",
    "        if os.path.exists(split_output_path):\n",
    "            print(f\"✓ {split_name} already exists, skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {split_name} split...\")\n",
    "\n",
    "        if split_name == \"train\":\n",
    "            # Use chunked processing for large train split\n",
    "            processed_split = process_in_chunks(\n",
    "                ds_dict[split_name],\n",
    "                split_name,\n",
    "                PROCESSED_DATASET_DIR,\n",
    "                chunk_size=10000  # ~10GB chunks\n",
    "            )\n",
    "        else:\n",
    "            # Regular processing for smaller splits\n",
    "            processed_split = ds_dict[split_name].map(\n",
    "                prepare_dataset_for_training,\n",
    "                batched=True,\n",
    "                batch_size=32,\n",
    "                num_proc=8,\n",
    "                keep_in_memory=False,\n",
    "                writer_batch_size=1000,\n",
    "            )\n",
    "\n",
    "        processed_split.save_to_disk(split_output_path)\n",
    "        print(f\"✓ {split_name} saved: {len(processed_split)} samples\")\n",
    "\n",
    "        del processed_split\n",
    "        gc.collect()\n",
    "\n",
    "    # Load the complete dataset\n",
    "    dataset = DatasetDict({\n",
    "        split: load_from_disk(os.path.join(PROCESSED_DATASET_DIR, split))\n",
    "        for split in splits\n",
    "    })\n",
    "    print(\"\\n✓ All splits processed and saved!\")\n",
    "\n",
    "print(f\"\\n✓ Train: {len(dataset['train'])} samples\")\n",
    "print(f\"✓ Validation: {len(dataset['validation'])} samples\")\n",
    "print(f\"✓ Test: {len(dataset['test'])} samples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample processed data:\")\n",
    "sample = dataset[\"train\"][0]\n",
    "print(f\"  Input features shape: {len(sample['input_features'])}\")\n",
    "print(f\"  Labels length: {len(sample['labels'])}\")\n",
    "print(f\"  First few label IDs: {sample['labels'][:10]}\")"
   ],
   "id": "516c48abee31afce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Found existing processed dataset at /root/uzbek-automatic-speech-recognition/outputs/part2/processed_uzbek_asr_dataset ---\n",
      "Loading from disk to save time...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/160 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "963de3879a99467687caebfdd554dfeb"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a467fe3c20146a1bc379996c6e89f4a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessed dataset loaded from disk!\n",
      "\n",
      "✓ Train: 66545 samples\n",
      "✓ Validation: 5728 samples\n",
      "✓ Test: 6994 samples\n",
      "\n",
      "Sample processed data:\n",
      "  Input features shape: 80\n",
      "  Labels length: 27\n",
      "  First few label IDs: [50258, 50337, 50359, 50363, 17648, 8588, 282, 350, 4727, 259]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:24:57.832180Z",
     "start_time": "2026-02-02T03:24:57.819670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad input features\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Pad labels\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding with -100 to ignore loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # Remove BOS token if present\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ],
   "id": "8988701ecc35207f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Base Model - Whisper Small",
   "id": "eb8a3cbf63780326"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T02:22:36.242987Z",
     "start_time": "2026-02-02T02:13:13.960452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scripts.whisper_utils import evaluate_by_dataset_with_trainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./base_model_eval_temp\",\n",
    "    per_device_eval_batch_size=256,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    fp16=True,\n",
    "    generation_num_beams=1,\n",
    "    dataloader_num_workers=8,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "evaluate_by_dataset_with_trainer(trainer, processor, dataset[\"test\"], \"test\")\n",
    "\n",
    "del model, trainer, processor\n",
    "gc.collect()"
   ],
   "id": "6eaf9979635da98e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "29246357e7d628587a570990b6e261c4"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: TEST\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)             116.27%\n",
      "CER (normalized)              45.34%\n",
      "Sequence Similarity           58.79%\n",
      "WER (raw)                    118.03%\n",
      "CER (raw)                     46.87%\n",
      "Seq Similarity (raw)          57.12%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)             116.25%\n",
      "CER (normalized)              41.51%\n",
      "Sequence Similarity           67.83%\n",
      "WER (raw)                    117.47%\n",
      "CER (raw)                     42.39%\n",
      "Seq Similarity (raw)          66.52%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)             114.97%\n",
      "CER (normalized)              60.47%\n",
      "Sequence Similarity           26.50%\n",
      "WER (raw)                    116.37%\n",
      "CER (raw)                     61.80%\n",
      "Seq Similarity (raw)          23.92%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)             116.49%\n",
      "CER (normalized)              59.64%\n",
      "Sequence Similarity           23.55%\n",
      "WER (raw)                    118.99%\n",
      "CER (raw)                     62.49%\n",
      "Seq Similarity (raw)          22.13%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)             116.39%\n",
      "CER (normalized)              40.48%\n",
      "Sequence Similarity           69.19%\n",
      "WER (raw)                    118.16%\n",
      "CER (raw)                     41.77%\n",
      "Seq Similarity (raw)          67.62%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OvozifyLabs/whisper-small-uz-v1",
   "id": "55a069963a420322"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:37:05.202203Z",
     "start_time": "2026-02-02T03:37:02.719649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=\"uz\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"OvozifyLabs/whisper-small-uz-v1\")\n",
    "\n",
    "model.generation_config.language = \"uz\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Force decoder to generate in Uzbek\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"uz\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# Set pad token to avoid attention mask warning\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "print(processor.feature_extractor)\n",
    "model"
   ],
   "id": "61b187ef21611ad9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"dither\": 0.0,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:34:09.074648Z",
     "start_time": "2026-02-02T03:25:55.924459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from scripts.whisper_utils import evaluate_by_dataset_with_trainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./base_model_eval_temp\",\n",
    "    per_device_eval_batch_size=256,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    fp16=True,\n",
    "    generation_num_beams=1,\n",
    "    dataloader_num_workers=8,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "evaluate_by_dataset_with_trainer(trainer, processor, dataset[\"test\"], \"test\")\n",
    "\n",
    "# del model, trainer, processor\n",
    "# gc.collect()"
   ],
   "id": "abed99e71a224c72",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "024d59a98d0ac46e54787ad1d19117bf"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: TEST\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               7.58%\n",
      "CER (normalized)               2.49%\n",
      "Sequence Similarity           94.93%\n",
      "WER (raw)                     34.50%\n",
      "CER (raw)                      6.42%\n",
      "Seq Similarity (raw)          88.22%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               6.83%\n",
      "CER (normalized)               1.68%\n",
      "Sequence Similarity           98.76%\n",
      "WER (raw)                     35.44%\n",
      "CER (raw)                      6.08%\n",
      "Seq Similarity (raw)          93.80%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              12.98%\n",
      "CER (normalized)               5.47%\n",
      "Sequence Similarity           84.89%\n",
      "WER (raw)                     28.21%\n",
      "CER (raw)                      8.02%\n",
      "Seq Similarity (raw)          72.84%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              18.14%\n",
      "CER (normalized)               7.09%\n",
      "Sequence Similarity           80.00%\n",
      "WER (raw)                     36.46%\n",
      "CER (raw)                     10.33%\n",
      "Seq Similarity (raw)          67.17%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               3.60%\n",
      "CER (normalized)               1.02%\n",
      "Sequence Similarity           99.34%\n",
      "WER (raw)                     34.37%\n",
      "CER (raw)                      5.14%\n",
      "Seq Similarity (raw)          94.59%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:39:00.873086Z",
     "start_time": "2026-02-02T03:39:00.856342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_model(model_path: str, audio_file: str):\n",
    "    \"\"\"\n",
    "    Test the fine-tuned model on a sample audio file\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to fine-tuned model\n",
    "        audio_file: Path to audio file to transcribe\n",
    "    \"\"\"\n",
    "\n",
    "    def group_words_by_sentences(chunks):\n",
    "        \"\"\"Group word-level timestamps into sentences\"\"\"\n",
    "        sentences = []\n",
    "        current_sentence = {'words': [], 'start': None, 'end': None, 'text': ''}\n",
    "\n",
    "        for chunk in chunks:\n",
    "            text = chunk['text'].strip()\n",
    "            start, end = chunk['timestamp']\n",
    "\n",
    "            # Initialize start time\n",
    "            if current_sentence['start'] is None:\n",
    "                current_sentence['start'] = start\n",
    "\n",
    "            current_sentence['words'].append(text)\n",
    "            current_sentence['end'] = end\n",
    "\n",
    "            # Check if sentence ends\n",
    "            if text.endswith('.') or text.endswith('!') or text.endswith('?'):\n",
    "                current_sentence['text'] = ' '.join(current_sentence['words'])\n",
    "                sentences.append(current_sentence.copy())\n",
    "                current_sentence = {'words': [], 'start': None, 'end': None, 'text': ''}\n",
    "\n",
    "        # Add remaining words as final sentence\n",
    "        if current_sentence['words']:\n",
    "            current_sentence['text'] = ' '.join(current_sentence['words'])\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TESTING MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Load model\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model_path,\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "    )\n",
    "\n",
    "    print(f\"✓ Loaded model from {model_path}\")\n",
    "    print(f\"✓ Processing: {audio_file}\")\n",
    "\n",
    "    # Transcribe\n",
    "    result = pipe(\n",
    "        audio_file,\n",
    "        language=\"uz\",\n",
    "        task=\"transcribe\",\n",
    "        return_timestamps=\"word\"\n",
    "    )\n",
    "    sentences = group_words_by_sentences(result['chunks'])\n",
    "\n",
    "    print(f\"\\nTranscription: {result['text']}\")\n",
    "    print(f\"\\nSentence-based timestamps:\")\n",
    "    for sent in sentences:\n",
    "        start_str = f\"{sent['start']:.2f}s\" if sent['start'] is not None else \"start\"\n",
    "        end_str = f\"{sent['end']:.2f}s\" if sent['end'] is not None else \"end\"\n",
    "        print(f\"[{start_str} - {end_str}]: {sent['text']}\")\n",
    "\n",
    "    return result"
   ],
   "id": "3d4278cb48c0539b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:42:03.884328Z",
     "start_time": "2026-02-02T03:41:56.339978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_audio = os.path.join(DATASET_DIR, \"devona_sample.wav\")\n",
    "if os.path.exists(test_audio):\n",
    "    test_model(\"OvozifyLabs/whisper-small-uz-v1\", test_audio)"
   ],
   "id": "cac745d39d1e7e89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d27a602f2d1a434daba76b59dc529f6c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09ab5176d63a4e04aeced2248031f8bb"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33752b0eb28e4da59187b48c75bb24c7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e6e1930ff9648e097ffb3fbfb034cc5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "adf4be8cd5144834b5be8c5da0499079"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/371 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dc46649be674b308e82fb0ededf3442"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model from OvozifyLabs/whisper-small-uz-v1\n",
      "✓ Processing: ../datasets/devona_sample.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription:  har kuni ertalab quyosh chiqayotganida uydan chiqargan. oldin eshik oldida turib atrofga uzoq tikilar. keyin qo‘llarini, ko‘zlarini va qalbini samoga yo‘naltirib, vadud, yo vadud deb baqirardi.\n",
      "\n",
      "Sentence-based timestamps:\n",
      "[0.00s - 4.40s]: har kuni ertalab quyosh chiqayotganida uydan chiqargan.\n",
      "[4.40s - 7.88s]: oldin eshik oldida turib atrofga uzoq tikilar.\n",
      "[7.88s - 14.98s]: keyin qo‘llarini, ko‘zlarini va qalbini samoga yo‘naltirib, vadud, yo vadud deb baqirardi.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## islomov/rubaistt_v2_medium",
   "id": "d8a7908a65d4e550"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T02:24:44.379254Z",
     "start_time": "2026-02-02T02:24:41.936617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(DATASET_PATH, index_col=\"id\", low_memory=False)\n",
    "\n",
    "# Shuffle dataset\n",
    "df = df.sample(frac=1, random_state=SEED)\n",
    "\n",
    "# Create full absolute path to audio\n",
    "df[\"path\"] = df.apply(\n",
    "    lambda row: os.path.abspath(\n",
    "        os.path.join(DATASET_DIR, row[\"dataset\"], \"sampled_audio\", row[\"path\"])\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "cols = [\"path\", \"type\", \"dataset\", \"duration\", \"ref_normalized\"]\n",
    "df = df[cols]\n",
    "\n",
    "# Make sure ref_normalized is never NaN\n",
    "# There was an exception thrown while processing dataset\n",
    "none_mask = df[\"ref_normalized\"].isna() | df[\"ref_normalized\"].isnull()\n",
    "df.loc[none_mask, \"ref_normalized\"] = \"\"\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"  Total duration: {df['duration'].sum() / 3600:.2f} hours\")\n",
    "print(f\"  Avg duration: {df['duration'].mean():.2f} seconds\")\n",
    "print(f\"  By Dataset:\")\n",
    "print((df.groupby([\"dataset\", \"type\"])[\"duration\"].sum() / 3600))\n",
    "print(f\"Total training samples: {len(df[df[\"type\"] == \"train\"]):,}\")\n",
    "print(f\"Total validation samples: {len(df[df[\"type\"] == \"validation\"]):,}\")\n",
    "print(f\"Total test samples: {len(df[df[\"type\"] == \"test\"]):,}\")\n",
    "\n",
    "df.to_csv(os.path.join(DATASET_DIR, \"combined_dataset_part2.csv\"), index_label=\"id\")\n",
    "df"
   ],
   "id": "127af2004314ec3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "  Total duration: 161.00 hours\n",
      "  Avg duration: 7.31 seconds\n",
      "  By Dataset:\n",
      "dataset        type      \n",
      "common_voice   test           2.751890\n",
      "               validation     1.328719\n",
      "feruza_speech  train          3.217020\n",
      "it             test           0.737151\n",
      "               train          7.940193\n",
      "               validation     0.735187\n",
      "news           test           1.726184\n",
      "               train         51.826843\n",
      "               validation     1.106672\n",
      "uzbek_voice    test           7.160320\n",
      "               train         75.527433\n",
      "               validation     6.939970\n",
      "Name: duration, dtype: float64\n",
      "Total training samples: 66,545\n",
      "Total validation samples: 5,728\n",
      "Total test samples: 6,994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                    path        type  \\\n",
       "id                                                                     \n",
       "69899  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "64879  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "6928   /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "24018  /root/uzbek-automatic-speech-recognition/datas...  validation   \n",
       "36042  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "...                                                  ...         ...   \n",
       "20665  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "35905  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "24573  /root/uzbek-automatic-speech-recognition/datas...  validation   \n",
       "44775  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "15562  /root/uzbek-automatic-speech-recognition/datas...       train   \n",
       "\n",
       "           dataset  duration  \\\n",
       "id                             \n",
       "69899         news    8.2418   \n",
       "64879         news    5.7762   \n",
       "6928   uzbek_voice    5.6160   \n",
       "24018  uzbek_voice    6.0840   \n",
       "36042  uzbek_voice    6.1136   \n",
       "...            ...       ...   \n",
       "20665  uzbek_voice    7.1280   \n",
       "35905  uzbek_voice    5.1304   \n",
       "24573  uzbek_voice    3.8160   \n",
       "44775  uzbek_voice    5.0760   \n",
       "15562  uzbek_voice    5.2560   \n",
       "\n",
       "                                          ref_normalized  \n",
       "id                                                        \n",
       "69899  Bir vaqtda sodir bo'ldi. Nega aynan shunday za...  \n",
       "64879  Rahmat berib o'tgan fikrlaringiz va ma'lumotla...  \n",
       "6928   O'yinchoqni olamiz, bolani qorni bilan fitbol ...  \n",
       "24018  E'londa kriptovalyutadagi narx vaqtincha belgi...  \n",
       "36042  Mazkur anjumanda iqlim bo'yicha yangi xalqaro ...  \n",
       "...                                                  ...  \n",
       "20665  Organizmda yo'qotilgan unsurning o'rnini to'ld...  \n",
       "35905  To'fon pasayganiga qaramay, xavf hali ham saql...  \n",
       "24573     Bu haqda hokimlik axborot xizmati xabar berdi.  \n",
       "44775  Shu yerda qishloq xo'jaligida amalga oshirilad...  \n",
       "15562  Balki bir muammomiz ikkita va undan ko'pga ort...  \n",
       "\n",
       "[79267 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>type</th>\n",
       "      <th>dataset</th>\n",
       "      <th>duration</th>\n",
       "      <th>ref_normalized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69899</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>news</td>\n",
       "      <td>8.2418</td>\n",
       "      <td>Bir vaqtda sodir bo'ldi. Nega aynan shunday za...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64879</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>news</td>\n",
       "      <td>5.7762</td>\n",
       "      <td>Rahmat berib o'tgan fikrlaringiz va ma'lumotla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6928</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>5.6160</td>\n",
       "      <td>O'yinchoqni olamiz, bolani qorni bilan fitbol ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24018</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>validation</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>6.0840</td>\n",
       "      <td>E'londa kriptovalyutadagi narx vaqtincha belgi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36042</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>6.1136</td>\n",
       "      <td>Mazkur anjumanda iqlim bo'yicha yangi xalqaro ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20665</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>7.1280</td>\n",
       "      <td>Organizmda yo'qotilgan unsurning o'rnini to'ld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35905</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>5.1304</td>\n",
       "      <td>To'fon pasayganiga qaramay, xavf hali ham saql...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24573</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>validation</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>3.8160</td>\n",
       "      <td>Bu haqda hokimlik axborot xizmati xabar berdi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44775</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>5.0760</td>\n",
       "      <td>Shu yerda qishloq xo'jaligida amalga oshirilad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15562</th>\n",
       "      <td>/root/uzbek-automatic-speech-recognition/datas...</td>\n",
       "      <td>train</td>\n",
       "      <td>uzbek_voice</td>\n",
       "      <td>5.2560</td>\n",
       "      <td>Balki bir muammomiz ikkita va undan ko'pga ort...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79267 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T02:25:03.703409Z",
     "start_time": "2026-02-02T02:25:03.632788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Audio, Dataset\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "ds_dict = DatasetDict({\n",
    "    \"test\": Dataset.from_pandas(df[df[\"type\"] == \"test\"]),\n",
    "})\n",
    "ds_dict = ds_dict.remove_columns([\"type\", \"duration\"])\n",
    "\n",
    "# Cast the path column to Audio\n",
    "ds_dict = ds_dict.cast_column(\"path\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# Rename columns for clarity\n",
    "ds_dict = ds_dict.rename_column(\"path\", \"audio\")\n",
    "\n",
    "ds_dict"
   ],
   "id": "718044f6a4787fa0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'dataset', 'ref_normalized', 'id'],\n",
       "        num_rows: 6994\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:42:44.344646Z",
     "start_time": "2026-02-02T03:42:41.900860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"uz\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"islomov/rubaistt_v2_medium\")\n",
    "\n",
    "model.generation_config.language = \"uz\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Force decoder to generate in Uzbek\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"uz\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# Set pad token to avoid attention mask warning\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "print(processor.feature_extractor)\n",
    "model"
   ],
   "id": "39ea4372587c16d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"dither\": 0.0,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T02:32:03.684236Z",
     "start_time": "2026-02-02T02:29:33.734699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NEW_PROCESSED_DATASET_DIR = os.path.join(\"../outputs/whisper_medium/processed_dataset\")\n",
    "\n",
    "\n",
    "def prepare_dataset_for_training(batch):\n",
    "    \"\"\"Preprocess a batch for training\"\"\"\n",
    "    # Load and process audio from the 'audio' column\n",
    "    audios = batch[\"audio\"]\n",
    "\n",
    "    # Compute log-Mel input features\n",
    "    input_features = processor.feature_extractor(\n",
    "        [audio[\"array\"] for audio in audios],\n",
    "        sampling_rate=audios[0][\"sampling_rate\"]\n",
    "    ).input_features\n",
    "\n",
    "    # Use __call__ method (faster) - just call the tokenizer directly\n",
    "    encoded = processor.tokenizer(\n",
    "        batch[\"ref_normalized\"],\n",
    "        truncation=True,\n",
    "        padding=False  # Don't pad here, let data collator handle it\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_features\": input_features,\n",
    "        \"labels\": encoded.input_ids,  # Extract input_ids from the result\n",
    "        \"dataset\": batch[\"dataset\"]  # As metadata for evaluation\n",
    "    }\n",
    "\n",
    "\n",
    "def process_in_chunks(dataset_split, split_name, output_dir, chunk_size=10000):\n",
    "    \"\"\"Process large dataset in chunks to avoid finalization OOM\"\"\"\n",
    "    num_samples = len(dataset_split)\n",
    "    num_chunks = (num_samples + chunk_size - 1) // chunk_size\n",
    "    chunk_dir = os.path.join(output_dir, f\"{split_name}_chunks\")\n",
    "    os.makedirs(chunk_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_path = os.path.join(chunk_dir, f\"chunk_{i}\")\n",
    "\n",
    "        if os.path.exists(chunk_path):\n",
    "            print(f\"  Chunk {i + 1}/{num_chunks} exists, skipping...\")\n",
    "            continue\n",
    "\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, num_samples)\n",
    "        print(f\"  Processing chunk {i + 1}/{num_chunks} (samples {start_idx}-{end_idx})\")\n",
    "\n",
    "        chunk = dataset_split.select(range(start_idx, end_idx))\n",
    "\n",
    "        processed_chunk = chunk.map(\n",
    "            prepare_dataset_for_training,\n",
    "            batched=True,\n",
    "            batch_size=64,\n",
    "            num_proc=4,\n",
    "            keep_in_memory=False,\n",
    "            writer_batch_size=1000,\n",
    "        )\n",
    "\n",
    "        processed_chunk.save_to_disk(chunk_path)\n",
    "\n",
    "        del chunk, processed_chunk\n",
    "        gc.collect()\n",
    "\n",
    "    # Now concatenate chunks (memory-mapped, should be safe)\n",
    "    print(f\"  Concatenating {num_chunks} chunks...\")\n",
    "    chunks = [\n",
    "        load_from_disk(os.path.join(chunk_dir, f\"chunk_{i}\"))\n",
    "        for i in range(num_chunks)\n",
    "    ]\n",
    "\n",
    "    # concatenate_datasets uses memory mapping, doesn't load everything\n",
    "    final_dataset = concatenate_datasets(chunks)\n",
    "\n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "# Check if fully processed\n",
    "splits = [\"test\"]\n",
    "all_exist = all(\n",
    "    os.path.exists(os.path.join(NEW_PROCESSED_DATASET_DIR, split))\n",
    "    for split in splits\n",
    ")\n",
    "\n",
    "if all_exist:\n",
    "    print(f\"--- Found existing processed dataset at {NEW_PROCESSED_DATASET_DIR} ---\")\n",
    "    print(\"Loading from disk to save time...\")\n",
    "    dataset = DatasetDict({\n",
    "        split: load_from_disk(os.path.join(NEW_PROCESSED_DATASET_DIR, split))\n",
    "        for split in splits\n",
    "    })\n",
    "    print(\"✓ Preprocessed dataset loaded from disk!\")\n",
    "else:\n",
    "    print(f\"--- Processed dataset not found or incomplete at {NEW_PROCESSED_DATASET_DIR} ---\")\n",
    "    print(\"Starting the heavy preprocessing (this will take a while)...\")\n",
    "    os.makedirs(NEW_PROCESSED_DATASET_DIR, exist_ok=True)\n",
    "\n",
    "    for split_name in [\"test\"]:\n",
    "        split_output_path = os.path.join(NEW_PROCESSED_DATASET_DIR, split_name)\n",
    "\n",
    "        if os.path.exists(split_output_path):\n",
    "            print(f\"✓ {split_name} already exists, skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {split_name} split...\")\n",
    "\n",
    "        if split_name == \"train\":\n",
    "            # Use chunked processing for large train split\n",
    "            processed_split = process_in_chunks(\n",
    "                ds_dict[split_name],\n",
    "                split_name,\n",
    "                NEW_PROCESSED_DATASET_DIR,\n",
    "                chunk_size=10000  # ~10GB chunks\n",
    "            )\n",
    "        else:\n",
    "            # Regular processing for smaller splits\n",
    "            processed_split = ds_dict[split_name].map(\n",
    "                prepare_dataset_for_training,\n",
    "                batched=True,\n",
    "                batch_size=32,\n",
    "                num_proc=8,\n",
    "                keep_in_memory=False,\n",
    "                writer_batch_size=1000,\n",
    "            )\n",
    "\n",
    "        processed_split.save_to_disk(split_output_path)\n",
    "        print(f\"✓ {split_name} saved: {len(processed_split)} samples\")\n",
    "\n",
    "        del processed_split\n",
    "        gc.collect()\n",
    "\n",
    "    # Load the complete dataset\n",
    "    dataset = DatasetDict({\n",
    "        split: load_from_disk(os.path.join(NEW_PROCESSED_DATASET_DIR, split))\n",
    "        for split in splits\n",
    "    })\n",
    "    print(\"\\n✓ All splits processed and saved!\")"
   ],
   "id": "3e8d181a918217a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processed dataset not found or incomplete at ../outputs/whisper_medium/processed_dataset ---\n",
      "Starting the heavy preprocessing (this will take a while)...\n",
      "\n",
      "Processing test split...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/6994 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7a174eaa58e44b3b8436a8624c0b312"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/17 shards):   0%|          | 0/6994 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f7c853780874e7e84d01ff0fee9cccb"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ test saved: 6994 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81d6a7bcaea041adaab2211668af899d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ All splits processed and saved!\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T02:55:39.088369Z",
     "start_time": "2026-02-02T02:34:44.620574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./base_model_eval_temp\",\n",
    "    per_device_eval_batch_size=192,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    fp16=True,\n",
    "    generation_num_beams=1,\n",
    "    dataloader_num_workers=8,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "evaluate_by_dataset_with_trainer(trainer, processor, dataset[\"test\"], \"test\")\n",
    "\n",
    "del model, trainer, processor\n",
    "gc.collect()"
   ],
   "id": "fe8cab7685bf6d82",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "fd16a9d38e26cd1d8177044d3dac7a68"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: TEST\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               7.33%\n",
      "CER (normalized)               1.76%\n",
      "Sequence Similarity           96.78%\n",
      "WER (raw)                     37.40%\n",
      "CER (raw)                      8.16%\n",
      "Seq Similarity (raw)          89.26%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               6.33%\n",
      "CER (normalized)               1.16%\n",
      "Sequence Similarity           99.14%\n",
      "WER (raw)                     31.73%\n",
      "CER (raw)                      4.84%\n",
      "Seq Similarity (raw)          96.29%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               9.33%\n",
      "CER (normalized)               2.84%\n",
      "Sequence Similarity           91.24%\n",
      "WER (raw)                     26.07%\n",
      "CER (raw)                      6.26%\n",
      "Seq Similarity (raw)          76.07%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              12.84%\n",
      "CER (normalized)               4.28%\n",
      "Sequence Similarity           88.27%\n",
      "WER (raw)                     33.03%\n",
      "CER (raw)                      9.59%\n",
      "Seq Similarity (raw)          71.70%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               5.59%\n",
      "CER (normalized)               1.07%\n",
      "Sequence Similarity           99.23%\n",
      "WER (raw)                     42.68%\n",
      "CER (raw)                      9.15%\n",
      "Seq Similarity (raw)          93.89%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:43:29.451696Z",
     "start_time": "2026-02-02T03:43:19.032775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_audio = os.path.join(DATASET_DIR, \"devona_sample.wav\")\n",
    "if os.path.exists(test_audio):\n",
    "    test_model(\"islomov/rubaistt_v2_medium\", test_audio)"
   ],
   "id": "a6c26402e4427e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13dd93345a29406ebf4a759cdf85f447"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca2b1f9c4fb641fabd81e0949e64d34c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b06f2ef50f2433e9551d40bbf03167b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a87fd0e93584ed9aa34d7bc79be5b00"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f55aba6417b840929bca417ec7bcca0a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e37ce94c0744c52971b3fe3792041a1"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/339 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a77a169156d43919e1af29564bac46b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model from islomov/rubaistt_v2_medium\n",
      "✓ Processing: ../datasets/devona_sample.wav\n",
      "\n",
      "Transcription: har kuni ertalab quyosh chiqayotganida uydan chiqarkan. oldin eshik oldida turib atrofga uzoq tikilar. keyin qo'llarini, ko'zlarini va qalbini samoga yo'naltirib, vadud, yo vadud deb baqirardi.\n",
      "\n",
      "Sentence-based timestamps:\n",
      "[0.00s - 4.46s]: har kuni ertalab quyosh chiqayotganida uydan chiqarkan.\n",
      "[4.46s - 7.88s]: oldin eshik oldida turib atrofga uzoq tikilar.\n",
      "[7.88s - 14.98s]: keyin qo'llarini, ko 'zlarini va qalbini samoga yo 'naltirib, vadud, yo vadud deb baqirardi.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Kotib/uzbek_stt_v1 (Whisper Medium)",
   "id": "dda01efb8b4257fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T02:57:32.371591Z",
     "start_time": "2026-02-02T02:57:29.086046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"uz\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"Kotib/uzbek_stt_v1\")\n",
    "\n",
    "model.generation_config.language = \"uz\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Force decoder to generate in Uzbek\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"uz\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# Set pad token to avoid attention mask warning\n",
    "if processor.tokenizer.pad_token_id is None:\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "print(processor.feature_extractor)\n",
    "model"
   ],
   "id": "f09c2f573c90d94d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"dither\": 0.0,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 80,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:19:07.351793Z",
     "start_time": "2026-02-02T02:57:37.131936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./base_model_eval_temp\",\n",
    "    per_device_eval_batch_size=192,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    fp16=True,\n",
    "    generation_num_beams=1,\n",
    "    dataloader_num_workers=8,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "evaluate_by_dataset_with_trainer(trainer, processor, dataset[\"test\"], \"test\")\n",
    "\n",
    "# del model, trainer, processor\n",
    "# gc.collect()"
   ],
   "id": "f80cc819a58eed18",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "7b58c3745d5261a8ac9f9c2121d3a255"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED EVALUATION: TEST\n",
      "================================================================================\n",
      "\n",
      "OVERALL METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               5.62%\n",
      "CER (normalized)               1.37%\n",
      "Sequence Similarity           96.93%\n",
      "WER (raw)                     28.01%\n",
      "CER (raw)                      6.77%\n",
      "Seq Similarity (raw)          89.89%\n",
      "\n",
      "================================================================================\n",
      "METRICS BY DATASET\n",
      "================================================================================\n",
      "\n",
      "common_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               5.28%\n",
      "CER (normalized)               0.99%\n",
      "Sequence Similarity           99.28%\n",
      "WER (raw)                     23.28%\n",
      "CER (raw)                      3.67%\n",
      "Seq Similarity (raw)          96.91%\n",
      "\n",
      "it\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               8.78%\n",
      "CER (normalized)               2.65%\n",
      "Sequence Similarity           91.88%\n",
      "WER (raw)                     25.18%\n",
      "CER (raw)                      6.03%\n",
      "Seq Similarity (raw)          76.91%\n",
      "\n",
      "news\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)              12.55%\n",
      "CER (normalized)               4.21%\n",
      "Sequence Similarity           87.38%\n",
      "WER (raw)                     32.97%\n",
      "CER (raw)                      9.49%\n",
      "Seq Similarity (raw)          70.39%\n",
      "\n",
      "uzbek_voice\n",
      "--------------------------------------------------------------------------------\n",
      "WER (normalized)               3.01%\n",
      "CER (normalized)               0.48%\n",
      "Sequence Similarity           99.65%\n",
      "WER (raw)                     28.56%\n",
      "CER (raw)                      7.13%\n",
      "Seq Similarity (raw)          95.05%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T03:44:23.070619Z",
     "start_time": "2026-02-02T03:44:11.079432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_audio = os.path.join(DATASET_DIR, \"devona_sample.wav\")\n",
    "if os.path.exists(test_audio):\n",
    "    test_model(\"Kotib/uzbek_stt_v1\", test_audio)"
   ],
   "id": "9a7c7eca047242ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f2187bc8ff34e9da4b2f5fef0fa3c4f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93e3c3995fe84ab2a4aba6fd6db04473"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6ff9fff5d1c4c49a9dbc901114e1717"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33c367d037d34b17a2e2ad6652cec42f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46cecb7ef2d742f7a0a75f411fbe047f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "655dee324d914e65b1c75f56aef8553e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/356 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32301b86f1354466ab17d28c057ad5d8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model from Kotib/uzbek_stt_v1\n",
      "✓ Processing: ../datasets/devona_sample.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription: har kuni ertalab quyosh chiqayotganda uydan chiqarkan. oldin eshik oldida turib atrofga uzoq tikilar. keyin qo'llarini, ko'zlarini va qalbini samoga yo'naltirib vadud, yo vadud deb baqirardi.\n",
      "\n",
      "Sentence-based timestamps:\n",
      "[0.00s - 4.40s]: har kuni ertalab quyosh chiqayotganda uydan chiqarkan.\n",
      "[4.40s - 7.88s]: oldin eshik oldida turib atrofga uzoq tikilar.\n",
      "[7.88s - 14.98s]: keyin qo'llarini, ko 'zlarini va qalbini samoga yo 'naltirib vadud, yo vadud deb baqirardi.\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
